{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f754539f",
   "metadata": {},
   "source": [
    "# End-To-End Data Pipeline\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook we'll be showing how the data is processed by the model from the initial sentence in the source language. We'll see how the tokenized representation of the sentence is generated and fed to the model, then follow this data as it is processed to generate the model's tokenized output in the target language, then finally perform a final mapping to get our target language representation.\n",
    "\n",
    "A diagram from the source paper, 'Attention is All You Need' Vaswani et al, is included below. The code that follows will be emulating how the data moves through the depicted model.\n",
    "\n",
    "![Encoder-Decoder Transformer Architecture](transformer_architecture.PNG)\n",
    "\n",
    "### Fetching the Input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29e26df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Madam President, on a point of order.\n",
      "\n",
      "Target: Madame la Présidente, c'est une motion de procédure.\n",
      "\n",
      "Token indexes: [2640, 748, 205, 284, 191, 769, 243, 1331, 207, 3]\n",
      "Padded tokens: ['▁madam', '▁president', '▁,', '▁on', '▁a', '▁point', '▁of', '▁order', '▁.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, torch, tokenizers, torch.nn as nn, matplotlib.pyplot as plt\n",
    "\n",
    "with open(f'{os.getcwd()}//data//europarl-v7.fr-en.en', mode='rt', encoding='utf-8') as f: # get source sentence\n",
    "        srcLine = f.readlines()[13]\n",
    "\n",
    "with open(f'{os.getcwd()}//data//europarl-v7.fr-en.fr', mode='rt', encoding='utf-8') as f: # get matching target sentence\n",
    "        tgtLine = f.readlines()[13]\n",
    "\n",
    "print(f\"Source: {srcLine}\")\n",
    "print(f\"Target: {tgtLine}\")\n",
    "\n",
    "tokenizer = tokenizers.Tokenizer.from_file(f\"data//europarl_en_fr_shared_tokenizer.json\")\n",
    "\n",
    "srcLine = re.sub(r'(\\w+)([.,?!])', r'\\1 \\2', srcLine).replace('\\n','').strip().lower() + '<eos>' # seperate puncuation from words\n",
    "tokens = tokenizer.encode(srcLine).ids # convert to embedding index\n",
    "\n",
    "print(f\"Token indexes: {tokens}\")\n",
    "valid_len = len(tokens)\n",
    "\n",
    "\n",
    "print(f\"Padded tokens: {[tokenizer.id_to_token(x) for x in tokens]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c86f80",
   "metadata": {},
   "source": [
    "Now we have a list of the indexes which map each word to their embeddings, and we can use this as an input to our transformer. First we load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd9ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgawalsh\\AppData\\Local\\Temp\\ipykernel_17804\\2593993492.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f'{os.getcwd()}//checkpoints//{modelName}_en-fr.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings:\n",
      "tensor([[[ 1.0138,  0.9699,  0.8692,  ..., -1.0086,  2.8476,  0.7290],\n",
      "         [-1.3306,  0.6401,  0.2260,  ..., -1.3788,  0.0470, -0.8041],\n",
      "         [-1.2621, -0.0741,  0.0187,  ..., -0.2106,  0.5816, -0.9406],\n",
      "         ...,\n",
      "         [ 0.7721, -0.5149, -0.2957,  ...,  0.9177,  0.9080, -0.9684],\n",
      "         [ 0.2074,  0.4447, -1.3799,  ...,  0.8991,  0.2173,  0.1851],\n",
      "         [-0.1093, -1.4123,  0.0588,  ..., -0.9406,  0.4999, -1.3812]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Shape:torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import model, settings, warnings\n",
    "\n",
    "modelName = \"Full\"\n",
    "params = settings.modelDict[modelName] # loading parameters for the full model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "encoder = model.TransformerEncoder(tokenizer.get_vocab_size(), params[\"num_hiddens\"], params[\"ffn_num_hiddens\"], params[\"num_heads\"], params[\"num_blks\"], params[\"dropout\"])\n",
    "decoder = model.TransformerDecoder(tokenizer.get_vocab_size(), params[\"num_hiddens\"], params[\"ffn_num_hiddens\"], params[\"num_heads\"], params[\"num_blks\"], params[\"dropout\"])\n",
    "myModel = model.Seq2Seq(encoder, decoder, tgt_pad=tokenizer.token_to_id('<pad>'))\n",
    "myModel.eval()\n",
    "\n",
    "state = torch.load(f'{os.getcwd()}//checkpoints//{modelName}_en-fr.pth')\n",
    "myModel.load_state_dict(state['model_state'])\n",
    "\n",
    "# Now we have our model available, and can recreate the process of how our model makes predictions by accessing the underlying layers indvidually.\n",
    "\n",
    "X = myModel.encoder.embedding(torch.tensor([tokens])) # wrap our tokens in a list to give us a batch size of one, then convert to embeddings\n",
    "print(f\"Embeddings:\\n{X}\\n\")\n",
    "print(f\"Shape:{X.shape}\")\n",
    "X = myModel.encoder.pos_encoding(X * math.sqrt(params[\"num_hiddens\"])) # Since positional encoding values are between -1 and 1, the embedding values are multiplied by the square root of the embedding dimension to rescale before they are summed up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bfa83",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "\n",
    "We now have our embeddings ready and can load up a transformer block and perform the self attention process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a6270b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Matrices shape: torch.Size([8, 10, 10])\n",
      "Repeated lengths:\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10])\n",
      "Head tensors shape: torch.Size([8, 10, 64])\n",
      "Concatenated heads shape: torch.Size([1, 10, 512])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myEncBlock = myModel.encoder.blks[0]\n",
    "\n",
    "norm_X = X\n",
    "\n",
    "def transpose_qkv(X):\n",
    "    # Shape of input X: (batch_size, no. of queries or key-value pairs, num_hiddens). Shape of output X: (batch_size, no. of queries or key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], params[\"num_heads\"], -1)\n",
    "    # Shape of output X: (batch_size, num_heads, no. of queries or key-value pairs, num_hiddens / num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # Shape of output: (batch_size * num_heads, no. of queries or key-value pairs, num_hiddens / num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "# we get our queries, keys, and values from the linear layers from each block\n",
    "queries = transpose_qkv(myEncBlock.attention.W_q(X))\n",
    "keys = transpose_qkv(myEncBlock.attention.W_k(X))\n",
    "values = transpose_qkv(myEncBlock.attention.W_v(X))\n",
    "\n",
    "X = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(queries.shape[-1]) # generate our intial unmasked attention matrix for each head\n",
    "shape = X.shape # save the shape so we can return to it later\n",
    "\n",
    "print(f\"Attention Matrices shape: {X.shape}\")\n",
    "X = X.reshape(-1, shape[-1])\n",
    "valid_lens = torch.repeat_interleave(torch.tensor([valid_len]), shape[1] * params[\"num_heads\"]) # generate our tensor for the masking process\n",
    "print(f\"Repeated lengths:\\n{valid_lens}\")\n",
    "\n",
    "mask = torch.arange(1, X.size(1) + 1, dtype=torch.float32, device=X.device)[None, :] > valid_lens[:, None] # Generates a 'mask' to be applied to our attention matrix, where all values above the valid length of our input sentence are True, and all values below are False \n",
    "X[mask] = -1e6 # On the last axis, replace masked elements with a very large negative value, whose exponentiation outputs 0\n",
    "X = nn.functional.softmax(X.reshape(shape), dim=-1) # cast back to original shape and apply softmax across last dimension\n",
    "\n",
    "plt.imshow(X[0].cpu().detach().numpy(), cmap = 'hot') # sample output attention matrix of our first head\n",
    "\n",
    "X = torch.bmm(X, values) # apply our final dot product attention function to re-establish our hidden dimension\n",
    "print(f\"Head tensors shape: {X.shape}\")\n",
    "\n",
    "# Now we perform a series of reshapings and permutations to concatenate the heads for each batch along the hidden dimension\n",
    "X = X.reshape(-1, params[\"num_heads\"], X.shape[1], X.shape[2])\n",
    "X = X.permute(0, 2, 1, 3)\n",
    "X = X.reshape(X.shape[0], X.shape[1], -1)\n",
    "print(f\"Concatenated heads shape: {X.shape}\")\n",
    "\n",
    "# We have now exited the self attention part of our block and can apply our final two layers, by adding a saved residual layer, and then performing another residual addition with the result and its output with a feed forward network\n",
    "X = myEncBlock.addnorm1(norm_X, X)\n",
    "X = myEncBlock.addnorm2(X, myEncBlock.ffn(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086803b",
   "metadata": {},
   "source": [
    "During that process we were able to output a graph showing our masked attention matrix. For each row, we see how much 'attention' one word pays to each of the other words in the sentence. The values beyond the valid_len limit have been zeroed by our mask on each row, but not on each column. However by applying the same process, we ensure those values will not be considered by the decoder.\n",
    "\n",
    "This is the end of a single transformer block process, which are fed into eachother in sequence for as many blocks as have been set by the parameters. We'll move onto the decoder process from here, and take our encoder output directly from our model's encoder.\n",
    "\n",
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51601b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder context shape: torch.Size([1, 10, 512])\n",
      "Initial output token index: [tensor([[2]])]\n"
     ]
    }
   ],
   "source": [
    "enc_valid_lens = torch.tensor([valid_len])\n",
    "\n",
    "enc_all_outputs = myModel.encoder(torch.tensor([tokens]), torch.tensor([enc_valid_lens]))\n",
    "print(f\"Encoder context shape: {enc_all_outputs.shape}\")\n",
    "\n",
    "dec_state = [enc_all_outputs, enc_valid_lens, [None] * myModel.decoder.num_blks] # save encoder output and valid lengths to be referenced throughout process\n",
    "\n",
    "outputs = [torch.full((enc_all_outputs.shape[0], 1), tokenizer.token_to_id('<bos>'))] # initialize output sentence with '<bos>' token\n",
    "print(f\"Initial output token index: {outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0ed59",
   "metadata": {},
   "source": [
    "The prediction loop consists of feeding the decoder the most recent output values. These values are used to generate a prediction which is added to the previous set of outputs. The loop can now be run again using the most recent output until the max length is reached. Each prediction cycle involves feeding the input through n blocks (set by the parameters of the model), where each block contains of two attention blocks. The first being a self attention block using only the predicted outputs up until that point, and the second combining the output of the first block with our values from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1eb04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predicted indicies: tensor([[1856,  235, 3688,  205,  197,  225,  318,  333, 3773,  216, 2291,  207,\n",
      "            3,    0,    0]])\n",
      "Translation: madame la présidente , c ' est une motion de procédure .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x173912da850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG1ZJREFUeJzt3X9s1IX9x/HX0dqjNO2x1tBy8wolIYKgqFSNgAJRuyDBsUWZIpVpskBWfpQaVpg6nRm91W1Mtw5I/UNYCJN/oDI2o52DAsEfpaXK3GZhNtDBms7F3JUCB7Sf7x/7ct9vpbQ9+jned+X5SO6P+/TT+7w/xevTz93nPvU4juMIAAADw6wHAABcv4gQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwk2o9wFd1d3fr1KlTyszMlMfjsR4HABAjx3HU0dEhv9+vYcP6PtZJuAidOnVKgUDAegwAwCC1trbqpptu6nOdhItQZmampP8On5WVFbft5Pl8cXtsALieOZLO6f9+n/cl4SJ06SW4rKysuEaIF/oAIL4G8pYKJyYAAMwQIQCAGSIEADBDhAAAZogQAMBM3CK0YcMGFRQUaPjw4Zo6dar2798fr00BAJJUXCK0fft2lZaW6rnnntPhw4d13333ac6cOTpx4kQ8NgcASFIex3Ectx/0nnvu0Z133qmNGzdGl02cOFHz589XMBjs83vD4bB8Pp9CoVBcPyeUwSWBACAuHElnpQH9Hnf9SOj8+fNqaGhQUVFRj+VFRUU6ePDgZetHIhGFw+EeNwDA9cH1CH3xxRfq6upSbm5uj+W5ublqa2u7bP1gMCifzxe9cd04ALh+xO3EhK9ersFxnF4v4bB27VqFQqHorbW1NV4jAQASjOvXjrvxxhuVkpJy2VFPe3v7ZUdHkuT1euX1et0eAwCQBFw/EkpLS9PUqVNVW1vbY3ltba2mTZvm9uYAAEksLlfRLisrU3FxsQoLC3XvvfequrpaJ06c0NKlS+OxOQBAkopLhL7zne/oP//5j15++WX961//0uTJk/XHP/5RY8aMicfmAABJKi6fExoMPicEAMnN9HNCAAAMFBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGdcjFAwGdddddykzM1OjRo3S/Pnz9dlnn7m9GQDAEOB6hOrq6lRSUqIPPvhAtbW1unjxooqKitTZ2en2pgAASc7jOI4Tzw38+9//1qhRo1RXV6f777+/3/XD4bB8Pp9CoZCysrLiNleGxxO3xwaA65kj6aw0oN/jqfEeJhQKSZKys7N7/XokElEkEoneD4fD8R4JAJAg4npiguM4Kisr04wZMzR58uRe1wkGg/L5fNFbIBCI50gAgAQS15fjSkpK9Ic//EEHDhzQTTfd1Os6vR0JBQIBXo4DgCSVEC/HLV++XLt27dK+ffuuGCBJ8nq98nq98RoDAJDAXI+Q4zhavny5du7cqb1796qgoMDtTQAAhgjXI1RSUqJt27bprbfeUmZmptra2iRJPp9P6enpbm8OAJDEXH9PyHOF91reeOMNffe73+33+zlFGwCSm+l7QnH+2BEAYAjh2nEAADNECABghggBAMwQIQCAGSIEADAT9wuYXq08n0+cRN23zmtwJiKnsgOIJ46EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMJNqPcCVtIVCysrKitvjZ3g8cXvsa2Uo7AOA6xtHQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwEzcIxQMBuXxeFRaWhrvTQEAkkxcI1RfX6/q6mrddttt8dwMACBJxS1Cp0+f1pNPPqnXX39dX/va1+K1GQBAEotbhEpKSjR37lw9+OCD8doEACDJxeXacW+++aYaGxtVX1/f77qRSESRSCR6PxwOx2MkAEACcv1IqLW1VStXrtTWrVs1fPjwftcPBoPy+XzRWyAQcHskAECC8jiO47j5gDU1NfrWt76llJSU6LKuri55PB4NGzZMkUikx9d6OxIKBAIKcRVtAEhKjqSz0oB+j7v+ctwDDzygI0eO9Fj29NNPa8KECSovL+8RIEnyer3yer1ujwEASAKuRygzM1OTJ0/usSwjI0M5OTmXLQcAXN+4YgIAwMw1+cuqe/fuvRabAQAkGY6EAABmiBAAwAwRAgCYIUIAADNECABg5pqcHXc18nw+JfM1DTrdvRBFr7jqA4Bkx5EQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZlKtBxiqMjwe6xHwvzodJ+7b4N8buDocCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgJi4ROnnypBYtWqScnByNGDFCt99+uxoaGuKxKQBAEnP9iglffvmlpk+frtmzZ+vtt9/WqFGj9I9//EMjR450e1MAgCTneoQqKysVCAT0xhtvRJeNHTvW7c0AAIYA11+O27VrlwoLC/XYY49p1KhRuuOOO/T6669fcf1IJKJwONzjBgC4Prgeoc8//1wbN27U+PHj9c4772jp0qVasWKFfvvb3/a6fjAYlM/ni94CgYDbIwEAEpTHcdy9xHBaWpoKCwt18ODB6LIVK1aovr5e77///mXrRyIRRSKR6P1wOKxAIKB0SVyXGG7gKtrAteVIOispFAopKyurz3VdPxIaPXq0brnllh7LJk6cqBMnTvS6vtfrVVZWVo8bAOD64HqEpk+frs8++6zHsubmZo0ZM8btTQEAkpzrEVq1apU++OADVVRU6NixY9q2bZuqq6tVUlLi9qYAAEnO9feEJGn37t1au3atjh49qoKCApWVlel73/vegL43HA7L5/PxnhBcw3tCwLUVy3tCcYnQYBAhuI0IAdeW6YkJAAAMFBECAJghQgAAM0QIAGCGCAEAzLh+FW38F2dkJQ5+TkDi4kgIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJhJtR7ASqfjxPXxMzyeuD4+AAwFHAkBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYMb1CF28eFHPP/+8CgoKlJ6ernHjxunll19Wd3e325sCACQ516+YUFlZqU2bNmnLli2aNGmSDh06pKefflo+n08rV650e3MAgCTmeoTef/99ffOb39TcuXMlSWPHjtXvfvc7HTp0yO1NAQCSnOsvx82YMUPvvfeempubJUkff/yxDhw4oIcffrjX9SORiMLhcI8bAOD64PqRUHl5uUKhkCZMmKCUlBR1dXVp3bp1euKJJ3pdPxgM6sc//rHbYwAAkoDrR0Lbt2/X1q1btW3bNjU2NmrLli36+c9/ri1btvS6/tq1axUKhaK31tZWt0cCACQoj+O4+zcNAoGA1qxZo5KSkuiyn/zkJ9q6dav+/ve/9/v94XBYPp9P6ZLi+ccQ+FMOABAfjqSzkkKhkLKysvpc1/UjoTNnzmjYsJ4Pm5KSwinaAIDLuP6e0Lx587Ru3Trl5+dr0qRJOnz4sNavX69nnnnG7U0BAJKc6y/HdXR06IUXXtDOnTvV3t4uv9+vJ554Qj/60Y+UlpbW7/fzchwAJLdYXo5zPUKDRYQAILmZvicEAMBAESEAgBkiBAAwQ4QAAGaIEADAjOufE0oWnL12/Yj3mZAS/z0BV4sjIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMykWg8wVHU6Tty3keHxxH0bQwE/JyBxcSQEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMxByhffv2ad68efL7/fJ4PKqpqenxdcdx9NJLL8nv9ys9PV2zZs3Sp59+6ta8AIAhJOYIdXZ2asqUKaqqqur166+88orWr1+vqqoq1dfXKy8vTw899JA6OjoGPSwAYGjxOM7Vf7Tf4/Fo586dmj9/vqT/HgX5/X6VlpaqvLxckhSJRJSbm6vKykotWbKk38cMh8Py+XxKl5TMn3PnigkArleOpLOSQqGQsrKy+lzX1feEWlpa1NbWpqKiougyr9ermTNn6uDBg71+TyQSUTgc7nEDAFwfXI1QW1ubJCk3N7fH8tzc3OjXvioYDMrn80VvgUDAzZEAAAksLmfHeb7yMpHjOJctu2Tt2rUKhULRW2trazxGAgAkIFevop2Xlyfpv0dEo0ePji5vb2+/7OjoEq/XK6/X6+YYAIAk4eqRUEFBgfLy8lRbWxtddv78edXV1WnatGlubgoAMATEfCR0+vRpHTt2LHq/paVFTU1Nys7OVn5+vkpLS1VRUaHx48dr/Pjxqqio0IgRI7Rw4UJXBwcAJL+YI3To0CHNnj07er+srEyStHjxYm3evFk/+MEPdPbsWX3/+9/Xl19+qXvuuUfvvvuuMjMz3ZsaADAkDOpzQvHA54QGjs8JAUhEZp8TAgAgFkQIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmUq0HGKoyPB7rEVzR6Thxffyh8nMCcHU4EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGZijtC+ffs0b948+f1+eTwe1dTURL924cIFlZeX69Zbb1VGRob8fr+eeuopnTp1ys2ZAQBDRMwR6uzs1JQpU1RVVXXZ186cOaPGxka98MILamxs1I4dO9Tc3KxHHnnElWEBAEOLx3Gu/iPxHo9HO3fu1Pz586+4Tn19ve6++24dP35c+fn5/T5mOByWz+dTuiQ+S2+PKyYAiJUj6aykUCikrKysPteN+3tCoVBIHo9HI0eOjPemAABJJq7Xjjt37pzWrFmjhQsXXrGGkUhEkUgkej8cDsdzJABAAonbkdCFCxf0+OOPq7u7Wxs2bLjiesFgUD6fL3oLBALxGgkAkGDiEqELFy5owYIFamlpUW1tbZ+vCa5du1ahUCh6a21tjcdIAIAE5PrLcZcCdPToUe3Zs0c5OTl9ru/1euX1et0eAwCQBGKO0OnTp3Xs2LHo/ZaWFjU1NSk7O1t+v1+PPvqoGhsbtXv3bnV1damtrU2SlJ2drbS0NPcmBwAkvZhP0d67d69mz5592fLFixfrpZdeUkFBQa/ft2fPHs2aNavfx+cU7cTCKdoAYhXLKdoxHwnNmjVLfXVrEB87AgBcZ7h2HADADBECAJghQgAAM0QIAGCGCAEAzBAhAICZuF7AdDDaBnB++WDw+ZSB4ecEIJ44EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADATMwR2rdvn+bNmye/3y+Px6OamporrrtkyRJ5PB69+uqrgxgRADBUxRyhzs5OTZkyRVVVVX2uV1NTow8//FB+v/+qhwMADG2psX7DnDlzNGfOnD7XOXnypJYtW6Z33nlHc+fOverhAABDW8wR6k93d7eKi4u1evVqTZo0qd/1I5GIIpFI9H44HHZ7JABAgnL9xITKykqlpqZqxYoVA1o/GAzK5/NFb4FAwO2RAAAJytUINTQ06LXXXtPmzZvl8XgG9D1r165VKBSK3lpbW90cCQCQwFyN0P79+9Xe3q78/HylpqYqNTVVx48f17PPPquxY8f2+j1er1dZWVk9bgCA64Or7wkVFxfrwQcf7LHsG9/4hoqLi/X000+7uSkAwBAQc4ROnz6tY8eORe+3tLSoqalJ2dnZys/PV05OTo/1b7jhBuXl5enmm28e/LQAgCEl5ggdOnRIs2fPjt4vKyuTJC1evFibN292bTAAwNDncRzHsR7i/wuHw/L5fAqFQnF9fyhjgCdOAABi40g6Kw3o9zjXjgMAmCFCAAAzRAgAYIYIAQDMuH7tuMG6dJ5EvK8hl1BnYwDAEHLp9+tAzntLuAh1dHRIEteQA4Ak19HRIZ/P1+c6CXeKdnd3t06dOqXMzMwBX38uHA4rEAiotbU1aS/7wz4kjqGwH+xDYhgK+yDFvh+O46ijo0N+v1/DhvX9rk/CHQkNGzZMN91001V971C49hz7kDiGwn6wD4lhKOyDFNt+9HcEdAknJgAAzBAhAICZIREhr9erF198UV6v13qUq8Y+JI6hsB/sQ2IYCvsgxXc/Eu7EBADA9WNIHAkBAJITEQIAmCFCAAAzRAgAYCbpI7RhwwYVFBRo+PDhmjp1qvbv3289UkyCwaDuuusuZWZmatSoUZo/f74+++wz67EGJRgMyuPxqLS01HqUmJw8eVKLFi1STk6ORowYodtvv10NDQ3WYw3YxYsX9fzzz6ugoEDp6ekaN26cXn75ZXV3d1uP1qd9+/Zp3rx58vv98ng8qqmp6fF1x3H00ksvye/3Kz09XbNmzdKnn35qM+wV9LUPFy5cUHl5uW699VZlZGTI7/frqaee0qlTp+wG7kV//w7/35IlS+TxePTqq68OertJHaHt27ertLRUzz33nA4fPqz77rtPc+bM0YkTJ6xHG7C6ujqVlJTogw8+UG1trS5evKiioiJ1dnZaj3ZV6uvrVV1drdtuu816lJh8+eWXmj59um644Qa9/fbb+utf/6pf/OIXGjlypPVoA1ZZWalNmzapqqpKf/vb3/TKK6/oZz/7mX79619bj9anzs5OTZkyRVVVVb1+/ZVXXtH69etVVVWl+vp65eXl6aGHHopeZzIR9LUPZ86cUWNjo1544QU1NjZqx44dam5u1iOPPGIw6ZX19+9wSU1NjT788EP5/X53NuwksbvvvttZunRpj2UTJkxw1qxZYzTR4LW3tzuSnLq6OutRYtbR0eGMHz/eqa2tdWbOnOmsXLnSeqQBKy8vd2bMmGE9xqDMnTvXeeaZZ3os+/a3v+0sWrTIaKLYSXJ27twZvd/d3e3k5eU5P/3pT6PLzp075/h8PmfTpk0GE/bvq/vQm48++siR5Bw/fvzaDBWjK+3DP//5T+frX/+685e//MUZM2aM88tf/nLQ20raI6Hz58+roaFBRUVFPZYXFRXp4MGDRlMNXigUkiRlZ2cbTxK7kpISzZ07Vw8++KD1KDHbtWuXCgsL9dhjj2nUqFG644479Prrr1uPFZMZM2bovffeU3NzsyTp448/1oEDB/Twww8bT3b1Wlpa1NbW1uN57vV6NXPmzKR/nns8nqQ60u7u7lZxcbFWr16tSZMmufa4CXcB04H64osv1NXVpdzc3B7Lc3Nz1dbWZjTV4DiOo7KyMs2YMUOTJ0+2Hicmb775phobG1VfX289ylX5/PPPtXHjRpWVlemHP/yhPvroI61YsUJer1dPPfWU9XgDUl5erlAopAkTJiglJUVdXV1at26dnnjiCevRrtql53Jvz/Pjx49bjDRo586d05o1a7Rw4cKkuqhpZWWlUlNTtWLFClcfN2kjdMlX/9yD4zgD/hMQiWbZsmX65JNPdODAAetRYtLa2qqVK1fq3Xff1fDhw63HuSrd3d0qLCxURUWFJOmOO+7Qp59+qo0bNyZNhLZv366tW7dq27ZtmjRpkpqamlRaWiq/36/FixdbjzcoQ+V5fuHCBT3++OPq7u7Whg0brMcZsIaGBr322mtqbGx0/eeetC/H3XjjjUpJSbnsqKe9vf2y/2tKBsuXL9euXbu0Z8+eq/5TFlYaGhrU3t6uqVOnKjU1Vampqaqrq9OvfvUrpaamqqury3rEfo0ePVq33HJLj2UTJ05MqpNcVq9erTVr1ujxxx/XrbfequLiYq1atUrBYNB6tKuWl5cnSUPieX7hwgUtWLBALS0tqq2tTaqjoP3796u9vV35+fnR5/jx48f17LPPauzYsYN67KSNUFpamqZOnara2toey2trazVt2jSjqWLnOI6WLVumHTt26M9//rMKCgqsR4rZAw88oCNHjqipqSl6Kyws1JNPPqmmpialpKRYj9iv6dOnX3ZqfHNzs8aMGWM0UezOnDlz2R8QS0lJSfhTtPtSUFCgvLy8Hs/z8+fPq66uLqme55cCdPToUf3pT39STk6O9UgxKS4u1ieffNLjOe73+7V69Wq98847g3rspH45rqysTMXFxSosLNS9996r6upqnThxQkuXLrUebcBKSkq0bds2vfXWW8rMzIz+H5/P51N6errxdAOTmZl52XtYGRkZysnJSZr3tlatWqVp06apoqJCCxYs0EcffaTq6mpVV1dbjzZg8+bN07p165Sfn69Jkybp8OHDWr9+vZ555hnr0fp0+vRpHTt2LHq/paVFTU1Nys7OVn5+vkpLS1VRUaHx48dr/Pjxqqio0IgRI7Rw4ULDqXvqax/8fr8effRRNTY2avfu3erq6oo+z7Ozs5WWlmY1dg/9/Tt8NZw33HCD8vLydPPNNw9uw4M+v87Yb37zG2fMmDFOWlqac+eddybdqc2Ser298cYb1qMNSrKdou04jvP73//emTx5suP1ep0JEyY41dXV1iPFJBwOOytXrnTy8/Od4cOHO+PGjXOee+45JxKJWI/Wpz179vT6HFi8eLHjOP89TfvFF1908vLyHK/X69x///3OkSNHbIf+ir72oaWl5YrP8z179liPHtXfv8NXuXWKNn/KAQBgJmnfEwIAJD8iBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwMz/AJjacwPwdreFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decoderEmulator(X, state):\n",
    "    X = decoder.pos_encoding(decoder.embedding(X) * math.sqrt(decoder.num_hiddens)) # inputs to embeddings\n",
    "    decoder._attention_weights = [[None] * len(decoder.blks) for _ in range (2)]\n",
    "\n",
    "    for i, blk in enumerate(decoder.blks):\n",
    "        X, state = decoderBlockEmulator(blk, X, state) # we pass 'None' for the decoder valid lengths as during prediction all decoder values are considered\n",
    "        decoder._attention_weights[0][i] = blk.attention1.attention.attention_weights # Decoder self-attention weights\n",
    "        # print(blk.attention1.attention.attention_weights)\n",
    "        if not i:\n",
    "            prediction_weights.append(decoder._attention_weights[0][i][0]) # appending our decoder self attention weights from the first head of the first attention function of the first block to be displayed later\n",
    "        decoder._attention_weights[1][i] = blk.attention2.attention.attention_weights # Encoder-decoder attention weights\n",
    "        # plt.imshow(decoder._attention_weights[1][i][0].cpu().detach().numpy(), cmap = 'hot')\n",
    "    \n",
    "    return decoder.dense(X), state\n",
    "\n",
    "def decoderBlockEmulator(myBlk, X, state):\n",
    "    enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "    # During training, all the tokens of any output sequence are processed at the same time, so state[2][self.i] is None as initialized. When decoding any output sequence token by token during prediction, state[2][self.i] contains representations of the decoded output at the i-th block up to the current time step\n",
    "    if state[2][myBlk.i] is None:\n",
    "        key_values = X\n",
    "    else:\n",
    "        key_values = torch.cat((state[2][myBlk.i], X), dim=1) #append preds\n",
    "    state[2][myBlk.i] = key_values\n",
    "    # Self-attention\n",
    "    X2 = myBlk.attention1(X, key_values, key_values, None)\n",
    "    Y = myBlk.addnorm1(X, X2)\n",
    "    # Encoder-decoder attention. Shape of enc_outputs:\n",
    "    # (batch_size, num_steps, num_hiddens)\n",
    "    Y2 = myBlk.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "    Z = myBlk.addnorm2(Y, Y2)\n",
    "    return myBlk.addnorm3(Z, myBlk.ffn(Z)), state\n",
    "\n",
    "prediction_weights = []\n",
    "target_length = round(valid_len * 1.5) # we set a maximum length for our predicted sentence to be twice the length of our source sentence\n",
    "\n",
    "for _ in range(target_length): \n",
    "    Y, dec_state = decoderEmulator(outputs[-1], dec_state) # latest predictions\n",
    "    outputs.append(torch.argmax(Y, 2)) # append predictions\n",
    "preds = torch.concat(outputs[1:], 1)\n",
    "\n",
    "# We now have our predicted outputs and can use our target vocabulary to generate the translation\n",
    "print(f\"Final predicted indicies: {preds}\")\n",
    "translation = tokenizer.decode(preds[0].tolist())\n",
    "print(f\"Translation: {translation}\")\n",
    "\n",
    "\n",
    "padded_vectors = [torch.nn.functional.pad(v[0], (0, target_length - v.size(1))) for v in prediction_weights] # Pad each vector to target_length\n",
    "\n",
    "out = torch.stack(padded_vectors)\n",
    "plt.imshow(out.cpu().detach().numpy(), cmap = 'hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a669e",
   "metadata": {},
   "source": [
    "Because only outputs that have been generated up until that point are available, only the lower left triangle of the resulting attention matrix can be populated as the upper right values are not yet available to be considered.\n",
    "\n",
    "Our final translation is generated by taking our output indexes and using our target vocabulary to generate the associated word tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
